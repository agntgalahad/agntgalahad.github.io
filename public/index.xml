<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome to My Site on Himanshu Patil</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Welcome to My Site on Himanshu Patil</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Feb 2025 23:42:11 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SLAM 101</title>
      <link>http://localhost:1313/blogs/slam_part_1/</link>
      <pubDate>Sat, 15 Feb 2025 23:42:11 +0530</pubDate>
      <guid>http://localhost:1313/blogs/slam_part_1/</guid>
      <description>&lt;p&gt;SLAM stands for Simultaneous Localization and Mapping. It has been and still is one of the unsolved problems in robotics.The main purpose of SLAM is for a robot to build the map of an unknown environment as well as know its position inside that environment&lt;br&gt;&#xA;Therefore SLAM tries to answer the 2 main question for a robot:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Where am I ?&lt;/li&gt;&#xA;&lt;li&gt;What does my surrounding look like ?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;SLAM can be divided into these core components:&lt;/p&gt;</description>
    </item>
    <item>
      <title>LiDAR-Equipped Autonomous Drone for 2D Mapping</title>
      <link>http://localhost:1313/projects/lidar_drone_project/</link>
      <pubDate>Fri, 31 Jan 2025 06:49:05 +0530</pubDate>
      <guid>http://localhost:1313/projects/lidar_drone_project/</guid>
      <description>&lt;p&gt;This project involves developing an autonomous drone equipped with a 2D LiDAR sensor (e.g., RPLiDAR) for real-time mapping and navigation. Using ROS2 Humble, the system integrates Computer Vision and Machine Learning to enhance obstacle detection, localization, and path planning. The 2D LiDAR enables the drone to perform 2D SLAM (Simultaneous Localization and Mapping), constructing a floor-level occupancy grid for navigation in indoor and structured environments. Additional onboard sensors, such as an IMU and a downward-facing camera, will assist in altitude estimation and stabilization. This project is designed to improve skills in robot perception, sensor fusion, and autonomous flight control, making it a valuable addition to research in aerial robotics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Update</title>
      <link>http://localhost:1313/blogs/update/</link>
      <pubDate>Mon, 27 Jan 2025 21:40:16 +0530</pubDate>
      <guid>http://localhost:1313/blogs/update/</guid>
      <description>&lt;p&gt;Yes this is a short post. I promise theres a big blog coming on one of the most interesting problems in robotics. So stay tuned for that.&#xA;So here is an update post on the project.&lt;/p&gt;&#xA;&lt;p&gt;We are trying to work out the payload capacity of the drone and therefore the thrust that is going to be reauired for successfully flying the drone.&#xA;We had it worked out when we ran into another problem. Initially we had planned to use just the LiDAR for mapping but as you ca see from my past posts that the LiDAR that we are using is a 2D LiDAR. The issue arises here that since we can see that the drone navigates in 3D thus plotting only 2D may not be the best aproach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>YD LiDAR Setup for ROS2 Humble</title>
      <link>http://localhost:1313/blogs/setup_ydlidar/</link>
      <pubDate>Mon, 20 Jan 2025 00:08:56 +0530</pubDate>
      <guid>http://localhost:1313/blogs/setup_ydlidar/</guid>
      <description>&lt;p&gt;The resources present on the internet for setting up the YDLidar SDK and integrating with ROS has been quite vague. So I am documenting the steps here for future reference.&lt;/p&gt;&#xA;&lt;p&gt;Here is my setup for reference:&lt;br&gt;&#xA;Ubuntu 22.04&lt;br&gt;&#xA;ROS2 Humble&lt;/p&gt;&#xA;&lt;p&gt;LiDAR Used: YDLidar X2&lt;/p&gt;&#xA;&lt;h1 id=&#34;yd-lidar-sdk-setup&#34;&gt;YD LiDAR SDK Setup&lt;/h1&gt;&#xA;&lt;p&gt;First of all we need to install the YDLidar SDK from their official github repo. The link for the same can be found here: &lt;a href=&#34;https://github.com/YDLIDAR/YDLidar-SDK&#34;&gt;YDLidar-SDK&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>The LiDAR Drone Idea</title>
      <link>http://localhost:1313/blogs/lidar_idea/</link>
      <pubDate>Wed, 15 Jan 2025 22:46:07 +0530</pubDate>
      <guid>http://localhost:1313/blogs/lidar_idea/</guid>
      <description>&lt;p&gt;IDEA:&lt;br&gt;&#xA;To mount a LiDAR sensor on top of a drone to allow for real time obstacle avoidance. This should allow to fly the drone in an indoor environment autonomously. The drone will also have a camera mounted on it for active person tracking and following while maintaining a safe distance.&lt;/p&gt;&#xA;&lt;p&gt;So in order to begin with the development we started with a drone frame and a LiDAR sensor. The LiDAR sensor used is a 2 dimensional LiDAR sensor which is capable of providing 360 degree scan of the environment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>M.E.A.P - Meta Enhanced A.I. Palette</title>
      <link>http://localhost:1313/projects/meap/</link>
      <pubDate>Wed, 10 Apr 2024 06:49:05 +0530</pubDate>
      <guid>http://localhost:1313/projects/meap/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;../../pics/Projects/meap_sample.png&#34; alt=&#34;SAMPLE&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement:&lt;/h2&gt;&#xA;&lt;p&gt;The landscape of image colorization tools is populated with solutions that predominantly rely on either manual interaction or automated algorithms. Automated algorithms leverage various techniques, including deep learning, to infer color from grayscale images. However, existing methods cannot often incorporate user guidance effectively, resulting in colorization that may not align with user expectations or preferences. This gap highlights the need for a more sophisticated approach that combines the power of generative models with user-provided textual prompts to produce accurate and personalized colorization.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Podkashvani</title>
      <link>http://localhost:1313/projects/podkashvani/</link>
      <pubDate>Fri, 01 Mar 2024 20:16:41 +0530</pubDate>
      <guid>http://localhost:1313/projects/podkashvani/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;../../pics/Projects/podkashvani_icon.png&#34; alt=&#34;PODKASHVANI ICON&#34;&gt;&#xA;CHECK OUT THE REPORT HERE &lt;a href=&#34;https://www.canva.com/design/DAF_uftzB5Y/OXqHvNV4Wu8P7rLPILUR_Q/view?utm_content=DAF_uftzB5Y&amp;amp;utm_campaign=designshare&amp;amp;utm_medium=link2&amp;amp;utm_source=uniquelinks&amp;amp;utlId=he1d8164373&#34;&gt;PODKASHVANI REPORT&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;project-description&#34;&gt;&lt;strong&gt;Project Description&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Podkashvani&lt;/strong&gt; is an AI-driven platform designed to revolutionize the way users consume textual information by converting dense documents into &lt;strong&gt;engaging audio podcasts&lt;/strong&gt;. This project leverages &lt;strong&gt;Natural Language Processing (NLP)&lt;/strong&gt; and &lt;strong&gt;Text-to-Speech (TTS) technologies&lt;/strong&gt; to break down complex academic content, research papers, and professional reports into easily digestible, audio-based learning modules.&lt;/p&gt;&#xA;&lt;p&gt;With the rise of digital learning, many students and professionals struggle with &lt;strong&gt;retaining focus&lt;/strong&gt; while reading long-form content. Podkashvani solves this problem by enabling users to &lt;strong&gt;listen and learn&lt;/strong&gt; anytime, anywhere, eliminating the barriers posed by traditional text-heavy study materials.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image - DeHaze</title>
      <link>http://localhost:1313/projects/image_dehaze/</link>
      <pubDate>Sun, 24 Sep 2023 06:49:05 +0530</pubDate>
      <guid>http://localhost:1313/projects/image_dehaze/</guid>
      <description>&lt;p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%;&#34;&gt;&#xA;    &lt;iframe src=&#34;https://www.youtube.com/embed/ftlFrpD1ex8?autoplay=1&#34; &#xA;        frameborder=&#34;0&#34; allowfullscreen &#xA;        style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34;&gt;&#xA;    &lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;Showcasing the hazy view (left) and the dehazed view (right)&lt;/p&gt;&#xA;&lt;h2 id=&#34;project-overview&#34;&gt;&lt;strong&gt;Project Overview&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;strong&gt;AI-ML Based Intelligent De-Smoking/De-Hazing Algorithm&lt;/strong&gt; is an advanced real-time image and video processing solution designed to enhance visibility in fire-prone environments. Developed as part of &lt;strong&gt;Smart India Hackathon (SIH) 2023&lt;/strong&gt;, this project focuses on improving rescue operations by providing clear visuals of areas affected by smoke and haze, particularly in indoor fire hazards.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GAN Anime Face Generator</title>
      <link>http://localhost:1313/projects/gan_anime_face_gen/</link>
      <pubDate>Mon, 16 Jan 2023 06:49:05 +0530</pubDate>
      <guid>http://localhost:1313/projects/gan_anime_face_gen/</guid>
      <description>&lt;p&gt;ðŸ“‚ &lt;strong&gt;GitHub Repository&lt;/strong&gt;: &lt;a href=&#34;https://github.com/agntgalahad/SIH2023-PixelEncoders&#34;&gt;SIH2023-PixelEncoders&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The GAN Anime Face Generator is a Deep Convolutional Generative Adversarial Network (DCGAN) designed to create 64x64 pixel anime-style face images from random noise inputs. Trained on a dataset of anime faces, the model comprises two primary components:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Generator&lt;/strong&gt;: This network takes random noise as input and generates new anime face images.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Discriminator&lt;/strong&gt;: This network evaluates images to distinguish between real images from the dataset and fake images produced by the generator.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
