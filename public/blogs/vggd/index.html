<!DOCTYPE html>
<html><head lang="en"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>VGGT: Visual Geometry Grounded Transformer - Himanshu Patil</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Introduction
3D reconstruction has always been one of the most interesting problems of 3D computer vision. The methodology until now had relied on Structure from Motion (SfM) and Multi View Sterio (MVS). These methodologies rely more on post processing algorithms and can only process 2 frames at a time.
VGGT (Visual Geometry Grounded Transformer) by Meta utilizes a large feed-forward transformer to infer all key 3D attributes directly from images. This eliminates the need for post-processing, making VGGT highly efficient while achieving state-of-the-art results in tasks like camera pose estimation, camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking." />
	<meta property="og:image" content=""/>
	<meta property="og:url" content="http://localhost:1313/blogs/vggd/">
  <meta property="og:site_name" content="Himanshu Patil">
  <meta property="og:title" content="VGGT: Visual Geometry Grounded Transformer">
  <meta property="og:description" content="Introduction 3D reconstruction has always been one of the most interesting problems of 3D computer vision. The methodology until now had relied on Structure from Motion (SfM) and Multi View Sterio (MVS). These methodologies rely more on post processing algorithms and can only process 2 frames at a time.
VGGT (Visual Geometry Grounded Transformer) by Meta utilizes a large feed-forward transformer to infer all key 3D attributes directly from images. This eliminates the need for post-processing, making VGGT highly efficient while achieving state-of-the-art results in tasks like camera pose estimation, camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-03-20T21:40:16+05:30">
    <meta property="article:modified_time" content="2025-03-20T21:40:16+05:30">
    <meta property="article:tag" content="Paper Review">
    <meta property="article:tag" content="Transformers">
    <meta property="article:tag" content="3d Computer Vision">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="VGGT: Visual Geometry Grounded Transformer">
  <meta name="twitter:description" content="Introduction 3D reconstruction has always been one of the most interesting problems of 3D computer vision. The methodology until now had relied on Structure from Motion (SfM) and Multi View Sterio (MVS). These methodologies rely more on post processing algorithms and can only process 2 frames at a time.
VGGT (Visual Geometry Grounded Transformer) by Meta utilizes a large feed-forward transformer to infer all key 3D attributes directly from images. This eliminates the need for post-processing, making VGGT highly efficient while achieving state-of-the-art results in tasks like camera pose estimation, camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking.">

        <link href="http://localhost:1313/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="http://localhost:1313/css/main.e5be0b244cfea0385bf04425148e0847f227ebc587eb7cf8ce8e2532d66a9248.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="http://localhost:1313/css/dark.50b57e12d401420df23965fed157368aba37b76df0ecefd0b1ecd4da664f01a0.css"   /><link rel="stylesheet" href="http://localhost:1313/katex/katex.min.css ">
		<script defer src="http://localhost:1313/katex/katex.min.js"></script>
		<script defer src="http://localhost:1313/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
		
		<script>
			document.addEventListener("DOMContentLoaded", function() {
					renderMathInElement(document.body, {
							delimiters: [
							{left: "$$", right: "$$", display: true},
        {left: "\\[", right: "\\]", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false}
							]
					});
			});
		</script>
	
    <link rel="stylesheet" href="http://localhost:1313/css/projects.min.06ce47d9454480c6c1e0580d327c3d4180e56cb3098dd631f08af9310a9a71d8.css">
	
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-0PDRRKVSZ7"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-0PDRRKVSZ7');
        }
      </script>
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="http://localhost:1313/">Himanshu Patil</a>
	</div>
	<nav>
		
		<a href="/">Home</a>
		
		<a href="/projects">Projects</a>
		
		<a href="/blogs">Blogs</a>
		
		<a href="/posts">All Posts</a>
		
		<a href="/tags">Tags</a>
		
		
	</nav>
</header>

<main>
  <article>
    <div class="post-container">
      
      
      <div class="post-content">
        <div class="title">
          <h1 class="title">VGGT: Visual Geometry Grounded Transformer</h1>
          <div class="meta">Posted on Mar 20, 2025</div>
        </div>
        
        <section class="body">
          <h1 id="introduction">Introduction</h1>
<p>3D reconstruction has always been one of the most interesting problems of 3D computer vision. The methodology until now had relied on Structure from Motion (SfM) and Multi View Sterio (MVS). These methodologies rely more on post processing algorithms and can only process 2 frames at a time.</p>
<p>VGGT (Visual Geometry Grounded Transformer) by Meta utilizes a large feed-forward transformer to infer all key 3D attributes directly from images. This eliminates the need for post-processing, making VGGT highly efficient while achieving state-of-the-art results in tasks like camera pose estimation, camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking.</p>
<h1 id="model-architecture">Model Architecture</h1>
<p><img src="../../pics/vggd/model_arch.png" alt="VGGD Model Architecture"></p>
<p><br>
<strong>PROBLEM DEFINITION</strong><br>
VGGT is a large feed-forward transformer designed to generate 3D scenes from 2D images. The model takes in N RGB images and predicts the 3D representation of the scene.</p>
<p>The problem definition can be formulated as</p>
<p>$$f((I_i)^N_{i=1}) = (g_i, D_i, P_i, T_i)_{i=1}^{N}$$</p>
<p>where:<br>
$g_i$ = Camera Parameters<br>
$D_i$ = depth map<br>
$P_i$ = point map<br>
$T_i$ = dense features extracted</p>
<p>All predictions are with respect to the coordinate frame of the 1st camera.</p>
<p>Here, $g_i$ can be defined as $g = [q,t,f]$ where $q$ is the rotation quaternion, $t$ is the translation vector and the field of view $f$.</p>
<p>The depth map is $D_i$ associates every pixel in the 2D image with the corresponding depth values as observed from the $i$th camera.</p>
<p>The point map $P_i$ associates each pixel with its corresponding 3D scene point.</p>
<p>The network uses an any keypoints selection method. So therefore, for any query image $I_q$ the network will give out a track that is formed by the corresponding 2D points $y_i$ in all the images $I_i$</p>
<p>$\mathcal{T}((y_q) = y^N_{i=1}$</p>
<p>Here the transformer doesnt output the tracks directly but the features $T_i$ that are used for tracking. The tracking is delegated to a separate function in the prediction head that takes in the the query points $y_q$ and dense tracking features $T_i$ from the transformer and compute the tracks.
Both the networks the transformer and tracking from th prediction head are trained jointly end to end.</p>
<p>One the interesting observations from the paper includes that even if the depth map can be deduced from the camera parameters and the point map, tasking the VGGT to predict these quantities during training increases the accuracy of the predictions. And during inference the prediction of these quantities allow for a more accurate point cloud as compared to using a separate point cloud branch.</p>
<p><br>
<strong>FEATURE BACKBONE</strong><br>
The model implemented in the VGGT is a large transformer. Every input image is patchified to a set of $K$ tokens. All the tokens from $N$ frames are then processed through the main network, alternating framewise attention and global self attention.</p>
<p><strong>Alternating Attention</strong><br>
Alternating Attention consists of 2 main parts:</p>
<ol>
<li>Frame wise Self Attention:
Computes the self attention of every token with respect to itself ie. in its own feature space. This allows the network to capture local spatial relationships for each frame.</li>
<li>Global Self Attention:
This block computes the inter frame relation by combining and refining features across views. This allows the model to understand relations across multiple views/frames allowing for a better point cloud generation and camera pose estimation.</li>
</ol>
<p>The alternating attention is the alternative arrangement of these blocks. The VGGT employs 24 layers of global and frame wise attention</p>
<p><br>
<strong>PREDICTION HEADS</strong><br>
Overall the model works by taking in an input image $I_i$ and then create the corresponding image tokens. An extra camera token $t_i^g$ is also added to the image tokens along with 4 register tokens $t_i^R$. Therefore the concatenation of $(t_i^I, t_i^g, t_i^Rj)^N_{i=1}$ is passed to the transformer.</p>
<p>Here the camera and register token from the 1st frame are set to a different learnable token in order to distinguish them from the rest since the frames rendered are in the frame of a the 1st camera.
The output register token is then discarded while the camera and image tokens are used for prediction.</p>
<p>For making the predicting the point clouds from the coordinate frame of the 1st camera are set to an identity matrix. The rotation quaternion and translation vector matrix is set to $[0,0,0,1]$ and $[0,0,0]$ respectively. This is possible due to the initial separation of the camera tokens of 1st camera.</p>
<p><strong>Camera Predictions</strong><br>
The model predicts the camera intrinsic and extrinsic parameters from the camera tokens using 4 additional self attention layers and a linear layer at the end.</p>
<p><strong>Dense Predictions</strong>
The output image token is used to predict the depth maps, point maps and the tracking features. The output image tokens are passed to the DPT layer to be converted to Dense Feature Map $F_i$. The $F_i$ is then passed to a 3x3 convolution layer to get the corresponding depth maps $D_i$ and point maps $P_i$. Additionally the dense features outputted by the DPT layer is used for tracking.</p>
<p><img src="../pics/vggd/dpt.png" alt="DPT Part of VGGT"></p>
<p>Note here that the aleatoric uncertainty is also predicted along with the depth maps and point clouds. Aleatoric uncertainty depicts the inherent uncertainty/ randomness in the prediction.</p>
<p><strong>Tracking</strong></p>

        </section>
        <div class="post-tags">
          
          
          <nav class="nav tags">
            <ul class="tags">
              
              <li><a href="/tags/paper-review">paper review</a></li>
              
              <li><a href="/tags/transformers">transformers</a></li>
              
              <li><a href="/tags/3d-computer-vision">3d computer vision</a></li>
              
            </ul>
          </nav>
          
          
        </div>
      </div>

      
      
      
    </div>

    <div id="disqus_thread"></div>
<script type="text/javascript">
    (function () {
        
        
        if (window.location.hostname == "localhost")
            return;

        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        var disqus_shortname = 'agntgalahad';
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
</article>
</main>
<footer>
  <div style="display:flex"><a class="soc" href="https://www.linkedin.com/in/himanshupatil829/" rel="me" title="LinkedIn"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#linkedin" />
</svg></a><a class="border"></a><a class="soc" href="https://github.com/agntgalahad" rel="me" title="GitHub"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#github" />
</svg></a><a class="border"></a><a class="soc" href="https://x.com/agnt_galahad" rel="me" title="Twitter"><svg class="feather">
   <use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#twitter" />
</svg></a><a class="border"></a></div>
  <div class="footer-info">
    2025  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>

</div>
    </body>
</html>
